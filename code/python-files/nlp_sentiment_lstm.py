# -*- coding: utf-8 -*-
"""nlp-sentiment-lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15w1w70Inaz9tpVtoFQbf1J5Ey_Zb2efs
"""

import numpy as np
import pandas as pd
import torch, string, re, nltk
import torch.nn as nn
import torch.nn.functional as F
from nltk.corpus import stopwords 
from collections import Counter
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
nltk.download('stopwords')

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
dataset_path = '/content/drive/Shareddrives/NLP/reviews_Video_Games_5.json'

df = pd.read_json(dataset_path, lines=True)
df.head()

texts, labels = df['reviewText'].values, df['overall'].values
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size = .5)
print(train_texts.shape, test_texts.shape)



dd = pd.Series(train_labels).value_counts()
sns.barplot(x=np.array([1, 2, 3, 4, 5]),y=dd.values)
plt.show()

def preprocess_string(s):
    s = re.sub(r"[^\w\s]", '', s)
    s = re.sub(r"\s+", '', s)
    s = re.sub(r"\d", '', s)
    return s

def make_tokenization(train_text, train_label, test_text, test_label):
    word_list = []

    stop_words = set(stopwords.words('english')) 
    for sent in train_texts:
        for word in sent.lower().split():
            word = preprocess_string(word)
            if word not in stop_words and word != '':
                word_list.append(word)

    corpus = Counter(word_list)
    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]
    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}

    final_list_train,final_list_test = [],[]
    for sent in train_text:
            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()  if preprocess_string(word) in onehot_dict.keys()])
    for sent in test_text:
            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()  if preprocess_string(word) in onehot_dict.keys()])
            
    encoded_train = [1 if label >= 3 else 0 for label in train_label]  
    encoded_test = [1 if label >= 3 else 0 for label in test_label] 
    return np.array(final_list_train), np.array(encoded_train), np.array(final_list_test), np.array(encoded_test), onehot_dict

train_texts, train_labels, test_texts, test_labels, vocab = make_tokenization(train_texts, train_labels, test_texts, test_labels)

def zero_padding(sentences, seq_len):
    features = np.zeros((len(sentences), seq_len),dtype=int)
    for ii, review in enumerate(sentences):
        if len(review) != 0:
            features[ii, -len(review):] = np.array(review)[:seq_len]
    return features

train_texts_pad = zero_padding(train_texts,500)
test_texts_pad = zero_padding(test_texts,500)
print(train_texts)

train_data = TensorDataset(torch.from_numpy(train_texts_pad), torch.from_numpy(train_labels))
test_data = TensorDataset(torch.from_numpy(test_texts_pad), torch.from_numpy(test_labels))

batch_size = 25

train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)
print(len(test_loader), len(train_loader))

class SentimentRNN(nn.Module):
    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):
        super(SentimentRNN,self).__init__()
 
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
 
        self.no_layers = no_layers
        self.vocab_size = vocab_size
    
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,
                           num_layers=no_layers, batch_first=True) # bidirectional = True could be added to enable bidirecional.
                           
        self.dropout = nn.Dropout(0.3)
    
        self.fc = nn.Linear(self.hidden_dim, output_dim)
        self.sig = nn.Sigmoid()
        
    def forward(self,x,hidden):
        batch_size = x.size(0)
        embeds = self.embedding(x)
        lstm_out, hidden = self.lstm(embeds, hidden)
        
        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) 
        
        out = self.dropout(lstm_out)
        out = self.fc(out)
        
        sig_out = self.sig(out)
        sig_out = sig_out.view(batch_size, -1)

        sig_out = sig_out[:, -1]    
        return sig_out, hidden
        
        
        
    def init_hidden(self, batch_size):
        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)
        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)
        hidden = (h0,c0)
        return hidden

no_layers = 2
vocab_size = len(vocab) + 1 #extra 1 for padding
embedding_dim = 64
output_dim = 1
hidden_dim = 256


model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)

#moving to gpu
model.to(device)
print(model)

lr=0.001

criterion = nn.BCELoss()

optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# function to predict accuracy
def acc(pred,label):
    pred = torch.round(pred.squeeze())
    return torch.sum(pred == label.squeeze()).item()
def calculate_stats(pred, label):
  pass

clip = 5
epochs = 5 
test_loss_min = np.Inf
# train for some number of epochs
epoch_tr_loss,epoch_tst_loss = [],[]
epoch_tr_acc,epoch_tst_acc = [],[]

for epoch in range(epochs):
    train_losses = []
    train_acc = 0.0
    model.train()
    h = model.init_hidden(batch_size)
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)   
        h = tuple([each.data for each in h])
        
        model.zero_grad()
        output,h = model(inputs,h)
        
        loss = criterion(output.squeeze(), labels.float())
        loss.backward()
        train_losses.append(loss.item())
        accuracy = acc(output,labels)
        train_acc += accuracy
        nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

    
        
    test_h = model.init_hidden(batch_size)
    test_losses = []
    test_acc = 0.0
    y_pred = []
    model.eval()
    for inputs, labels in test_loader:
        test_h = tuple([each.data for each in test_h])
        inputs, labels = inputs.to(device), labels.to(device)
        output, test_h = model(inputs, test_h)
        test_loss = criterion(output.squeeze(), labels.float())
        test_losses.append(test_loss.item())
        accuracy = acc(output,labels)
        test_acc += accuracy
        y_pred += torch.round(output).tolist()

            
    epoch_train_loss, epoch_test_loss = np.mean(train_losses), np.mean(test_losses)
    epoch_train_acc, epoch_test_acc = train_acc/len(train_loader.dataset), test_acc/len(test_loader.dataset)
    
    
    print(classification_report(test_labels[:len(y_pred)], y_pred))
    print(f'train_accuracy : {epoch_train_acc*100}')
    epoch_tr_loss.append(epoch_train_loss)
    epoch_tst_loss.append(epoch_test_loss)
    epoch_tr_acc.append(epoch_train_acc)
    epoch_tst_acc.append(epoch_test_acc)
    print(f'Epoch {epoch+1}')
    print(f'train_loss : {epoch_train_loss} test_loss : {epoch_test_loss}')
    if epoch_test_loss <= test_loss_min:
        torch.save(model.state_dict(), 'state_dict.pt')
        print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(test_loss_min,epoch_test_loss))
        test_loss_min = epoch_test_loss
    print(25*'==')

epoch_tst_acc = np.insert(epoch_tst_acc, 0, 0)
epoch_tr_acc = np.insert(epoch_tr_acc, 0, 0)
epoch_tst_acc, epoch_tr_acc

fig = plt.figure(figsize = (10, 6))
plt.plot(epoch_tr_acc, label='Train Acc')
plt.plot(epoch_tst_acc, label='Test Acc')
plt.xlim(1,5,1)
plt.ylim(0.9, 1, 0.01)
plt.title("Accuracy")
plt.legend()
plt.grid()
plt.show()

def predict_sentiment(text):
        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() 
                         if preprocess_string(word) in vocab.keys()])
        word_seq = np.expand_dims(word_seq,axis=0)
        pad =  torch.from_numpy(zero_padding(word_seq,500))
        inputs = pad.to(device)
        batch_size = 1
        h = model.init_hidden(batch_size)
        h = tuple([each.data for each in h])
        output, h = model(inputs, h)
        return(output.item())

def print_results(index):
  print(df['reviewText'][index])
  print('='*70)
  print('Actual ratio is {} which is {}'.format(df["overall"][index], 'positive' if df["overall"][index] >= 3 else 'negative'))
  print('='*70)
  probability = predict_sentiment(df['reviewText'][index])
  status = "positive" if probability > 0.5 else "negative"
  probability = (1 - probability) if status == "negative" else probability
  print(f'Predicted sentiment is {status} with a probability of {probability}')

index = 25
print_results(index)